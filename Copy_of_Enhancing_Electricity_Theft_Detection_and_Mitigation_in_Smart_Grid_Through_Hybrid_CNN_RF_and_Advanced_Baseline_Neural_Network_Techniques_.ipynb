{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bambillo1/elecomsCoding_academy/blob/main/Copy_of_Enhancing_Electricity_Theft_Detection_and_Mitigation_in_Smart_Grid_Through_Hybrid_CNN_RF_and_Advanced_Baseline_Neural_Network_Techniques_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqaJZGpYstcC"
      },
      "outputs": [],
      "source": [
        "#Libraries import\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "import matplotlib as mp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Activation, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,CSVLogger, LearningRateScheduler,ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import binary_crossentropy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, BinaryAccuracy, Precision, Recall, AUC\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#from plot_keras_history import show_history, plot_history\n",
        "#from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "#!pip install scikeras\n",
        "#!pip install plot-keras-history\n",
        "#from scikeras.wrappers import KerasClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CukpPjlItcrz"
      },
      "outputs": [],
      "source": [
        "#Setting parameters for plots\n",
        "mp.rcParams['figure.figsize'] = (12, 10)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqhv_1NnCGkW"
      },
      "source": [
        "#Mounting data on gdrive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e__o6ybDtdBa"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U91h-kS6CeUv"
      },
      "source": [
        "Link data residing in gdrive to colab using this link from the data(in gdrive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orDQpF3ytdKM"
      },
      "outputs": [],
      "source": [
        "!gdown  --id 1OIz8U3aymDeGDUIl83vyTFCPyfLulSgM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjkSeB_EC2Z5"
      },
      "source": [
        "Using read_csv to read in data into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2_QNXTVrtdTl",
        "outputId": "49fdcb52-276e-4c11-f647-2bb216a2b98d"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6456521db808>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNVcdBnDCUZ"
      },
      "source": [
        "#Proportion of flag (1) and not flag(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WFtd01VdtdbC"
      },
      "outputs": [],
      "source": [
        "#Proportion of flag (1) and not flag(0)\n",
        "\n",
        "one = df[df['FLAG'] == 1].shape[0]\n",
        "zero = df[df['FLAG'] == 0].shape[0]\n",
        "\n",
        "total = zero + one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DxDPP-TowUtG"
      },
      "outputs": [],
      "source": [
        "total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ndYd5OeRtdkv"
      },
      "outputs": [],
      "source": [
        "print(one / (one + zero) * 100, '% of customers flagged.')\n",
        "\n",
        "print(zero / (one + zero) * 100, '% of customers unflagged.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdaTjOW-DJ_f"
      },
      "source": [
        "Visualize data using pie chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VdKUBMnUwa3H"
      },
      "outputs": [],
      "source": [
        "df[\"FLAG\"].value_counts().plot(kind = 'pie',explode=[0, 0.1],figsize=(6, 6),autopct='%1.1f%%',shadow=True)\n",
        "plt.title(\"Fraudulent and Non-Fraudulent Distribution\",fontsize=20)\n",
        "plt.legend([\"zero\", \"one\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUJ9IwRSEAcc"
      },
      "source": [
        "Check first 10 rows of the df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "veWvnYIPwciY"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peaHzzaEEWFy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5id10a0SEIDY"
      },
      "source": [
        "Append other columns except for 'CONS_NO','FLAG' columns into a list named Ib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bICLksChwcZW"
      },
      "outputs": [],
      "source": [
        "l=df.columns\n",
        "la=['CONS_NO','FLAG']\n",
        "lb=[]\n",
        "for i in l:\n",
        "    if i not in la:\n",
        "        lb.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip1aFk91EkZc"
      },
      "source": [
        "Format the date in year/month/day mode for all columns and store them in a list fdatesdates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4ZBY_FyhwcQs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import datetime\n",
        "dates = [datetime.datetime.strptime(ts, \"%Y/%m/%d\") for ts in lb]\n",
        "#dates.sort()\n",
        "fdatesdates = [datetime.datetime.strftime(ts, \"%Y/%m/%d\") for ts in dates]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o-ZVarKE_Bh"
      },
      "source": [
        "Insert 0 in all rows of column FLAG, same with CON_NO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M3FOQ8o1wcHM"
      },
      "outputs": [],
      "source": [
        "fdatesdates.insert(0,\"FLAG\")\n",
        "fdatesdates.insert(0,\"CONS_NO\")\n",
        "\n",
        "df.columns=fdatesdates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apyxCek3FX1D"
      },
      "source": [
        "Using sort method to sort dates in ascending order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZK2l5kkkwb9J"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "dates = [datetime.datetime.strptime(ts, \"%Y/%m/%d\") for ts in lb]\n",
        "dates.sort()\n",
        "sorteddates = [datetime.datetime.strftime(ts, \"%Y/%m/%d\") for ts in dates]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPq2Jlc0FijW"
      },
      "source": [
        "Concatenate sorted dates AND columns CONS_NO and FLAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aYbxhOeOwb1w"
      },
      "outputs": [],
      "source": [
        "cols=df.columns.tolist()[0:2]+sorteddates\n",
        "df=df[cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5daa7rSGDAT"
      },
      "source": [
        "Fill all columns with there respective observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GZzEwWvjwbvA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "train_df=df\n",
        "l=train_df[\"2014/01/01\"]\n",
        "l1=train_df[\"2014/01/01\"]\n",
        "l=np.asarray(l).tolist()\n",
        "l1=np.asarray(l1).tolist()\n",
        "\n",
        "l2=[]\n",
        "for i in range(len(l)):\n",
        "    if math.isnan(l[i]):\n",
        "        if math.isnan(l1[i]):\n",
        "            l2.append(0)\n",
        "        else:\n",
        "            l2.append(l1[i]/2)\n",
        "    else:\n",
        "        l2.append(l[i])\n",
        "train_df[\"2014/01/01\"]=l2\n",
        "\n",
        "train_df.head()\n",
        "l=train_df[\"2016/10/31\"]\n",
        "l1=train_df[\"2016/10/31\"]\n",
        "l=np.asarray(l).tolist()\n",
        "l1=np.asarray(l1).tolist()\n",
        "\n",
        "l2=[]\n",
        "for i in range(len(l)):\n",
        "    if math.isnan(l[i]):\n",
        "        if math.isnan(l1[i]):\n",
        "            l2.append(0)\n",
        "        else:\n",
        "            l2.append(l1[i]/2)\n",
        "    else:\n",
        "        l2.append(l[i])\n",
        "train_df[\"2016/10/31\"]=l2\n",
        "\n",
        "\n",
        "l=train_df.columns\n",
        "la=['CONS_NO','FLAG']\n",
        "lbx=[]\n",
        "for i in l:\n",
        "    if i not in la:\n",
        "        lbx.append(i)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhg-_PQaGgF7"
      },
      "source": [
        "CNS_NO is a categorical columns that contains string objects representing each customer. It is of no use in the dataframe for modelling, thus, dropped or removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8mcrZDnrwbo3"
      },
      "outputs": [],
      "source": [
        "df = df.drop('CONS_NO', axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS8YkqPaHTAd"
      },
      "source": [
        "Using interpolate method for replacing missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uDQcGIdUxIq8"
      },
      "outputs": [],
      "source": [
        "df=df.interpolate(method ='linear', limit_direction ='forward')\n",
        "df=df.interpolate(method ='linear', limit_direction ='backward')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsYiBTKJHeru"
      },
      "source": [
        "Splitting data into train, test and validation sets with test size of 20%, validation 20% and train of 60%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nscFrfgBxIbF"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.2, stratify = df['FLAG'])\n",
        "df_train, df_val = train_test_split(df, test_size=0.2, stratify = df['FLAG'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TvJ9ivlxxIIi"
      },
      "outputs": [],
      "source": [
        "#Check first 5 rows of df\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0ETz2HIPxH-y"
      },
      "outputs": [],
      "source": [
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ih2JdE7VxH19"
      },
      "outputs": [],
      "source": [
        "df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rrJ-AXpixHvA"
      },
      "outputs": [],
      "source": [
        "# Convert dataframe into  numpy arrays of labels and features.\n",
        "df_train_labels = np.array(df_train.pop('FLAG'))   #pop function means remove FLAG from the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iP8eSFONxHlt"
      },
      "outputs": [],
      "source": [
        "df_val_labels = np.array(df_val.pop('FLAG'))  #pop function means remove FLAG from the val data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XxjMVewhxHag"
      },
      "outputs": [],
      "source": [
        "df_test_labels = np.array(df_test.pop('FLAG'))#pop function means remove FLAG from the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LYWDfMP-xHRF"
      },
      "outputs": [],
      "source": [
        "bin_train_labels = df_train_labels != 0 #Where df_train_labels is not equal to zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S3EleQh4xHHy"
      },
      "outputs": [],
      "source": [
        "df_train_features = np.array(df_train) #This is now  a numpy array of train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CRTbrr7UxG_q"
      },
      "outputs": [],
      "source": [
        "df_val_features = np.array(df_val)   ##This is now  a numpy array of val data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wn_mcZiWwbiu"
      },
      "outputs": [],
      "source": [
        "df_test_features = np.array(df_test)#This is now  a numpy array of test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uBW8aXEkzMi2"
      },
      "outputs": [],
      "source": [
        "df_train_features.shape #Check no of rows and columns of train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEjIFL73J-DL"
      },
      "source": [
        "Use MinMaxScaler to normalize all features i -: Uniform Scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ba8UdFbjzL8R"
      },
      "outputs": [],
      "source": [
        "scale_features = MinMaxScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SaHwXZ3XzLv1"
      },
      "outputs": [],
      "source": [
        "df_train_features = scale_features.fit_transform(df_train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "klXDtlfEzLll"
      },
      "outputs": [],
      "source": [
        "df_val_features = scale_features.transform(df_val_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bJ5ypD1JzLaa"
      },
      "outputs": [],
      "source": [
        "df_test_features = scale_features.transform(df_test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MkWYrm0pzLQR"
      },
      "outputs": [],
      "source": [
        "df_train_features = np.clip(df_train_features, -5, 5) #Remove any observation less and equal to -5 and less than and greater than 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6SW-BWV4zLIN"
      },
      "outputs": [],
      "source": [
        "df_val_features = np.clip(df_val_features, -5, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dxGiAEoyzK-9"
      },
      "outputs": [],
      "source": [
        "df_test_features = np.clip(df_test_features, -5, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MUicrxsdzK3b"
      },
      "outputs": [],
      "source": [
        "print(' Shape of training labels: ', df_train_labels.shape)\n",
        "print(' Shape of validation labels: ', df_val_labels.shape)\n",
        "print(' Shape of test labels: ', df_test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbAU1pKfLF5p"
      },
      "source": [
        "Print the shapes of all features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DvGph3V2zKv7"
      },
      "outputs": [],
      "source": [
        "print('Shape of training features : ', df_train_features.shape)\n",
        "print('Shape of validation features: ', df_val_features.shape)\n",
        "print('Shape of test features: ', df_test_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRAZrEsLN9d"
      },
      "source": [
        "Build a function for performance metrics and baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BjwO70jLzKpE"
      },
      "outputs": [],
      "source": [
        "METRICS = [\n",
        "      TruePositives(name='tp'),\n",
        "      FalsePositives(name='fp'),\n",
        "      TrueNegatives(name='tn'),\n",
        "      FalseNegatives(name='fn'),\n",
        "      BinaryAccuracy(name='accuracy'),\n",
        "      Precision(name='precision'),\n",
        "      Recall(name='recall'),\n",
        "      AUC(name='auc'),\n",
        "      AUC(name='prc', curve='PR'),\n",
        "                        ]\n",
        "\n",
        "def make_model(metrics=METRICS, output_bias=None):\n",
        "\n",
        "  if output_bias is not None:\n",
        "\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "\n",
        "  model = keras.Sequential([keras.layers.Dense(32, activation='relu',input_shape=(df_train_features.shape[-1],)),\n",
        "\n",
        "      keras.layers.Flatten(),\n",
        "\n",
        "      keras.layers.Dense(32, activation = 'relu', kernel_initializer = 'he_uniform'),\n",
        "\n",
        "      keras.layers.Dropout(0.7),\n",
        "\n",
        "      keras.layers.Dense(64, activation = 'relu', kernel_initializer = 'he_uniform'),\n",
        "\n",
        "      keras.layers.Dropout(0.7),\n",
        "\n",
        "      keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias),\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=metrics)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMwKlpIw0NxQ"
      },
      "source": [
        "Notice that the model is fit using a larger than default batch size of 2048, this is important to ensure that each batch has a decent chance of containing a few positive samples. If the batch size was too small, they would likely have no fraudulent transactions to learn from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GA8wMCv0EM9"
      },
      "source": [
        "#Baseline Model\n",
        "early_stopping is when there is convergence during training-It means training stops where same scores are being repeated at every epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sRR9Z61p0Ah7"
      },
      "outputs": [],
      "source": [
        "epochs = 100  # no of iterations the neurons tranverses the network\n",
        "batch_size = 2048 # Each batch of data that are entering the network at a time\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_prc', verbose=1, patience=10, mode='max', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ouHrSNl00AUF"
      },
      "outputs": [],
      "source": [
        "model = make_model() # invoking make_model function created above\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PL3bTU30qkb"
      },
      "source": [
        "We are now predicting the probabilites using the intial weights assigned and the initial predictions are not that generalized. Many of the values are predicted as fraud transactions (probability > 0.5). We can see that the class imbalance is not learnt by the model and it will take a few epochs for the model to learn that there are very less fraud transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3SPJFQHR0AKZ"
      },
      "outputs": [],
      "source": [
        "model.predict(df_train_features[:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgS44kpW04wz"
      },
      "source": [
        "#Understanding Initial Loss\n",
        "Evaluate the model of training data and check the initial loss during training.\n",
        "When initializing the biases of the neural network's final layer (the layer connected to the sigmoid activation) to their default values,\n",
        "it is often set such that the initial predicted probability is around 0.5 for both classes. This makes sense because, in the beginning,\n",
        "the model has no knowledge about the data distribution, and it's a reasonable assumption that both classes are equally likely.\n",
        "According to this if for one sample we assume the probability as 0.5 and calculate the loss using binary crossentropy, out initial loss should be:\n",
        "log(2) = 0.69314"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N7rucAem0AAm"
      },
      "outputs": [],
      "source": [
        "returns = model.evaluate(df_train_features, df_train_labels, batch_size=batch_size, verbose=0)\n",
        "print(\"The initial model loss is {:0.5f}\".format(returns[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjERD6cASC0n"
      },
      "source": [
        "Bias of the model is the logarithm of proportion of positive divided by the negative  class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iNQZ_Mhcz_3a"
      },
      "outputs": [],
      "source": [
        "first_bias = np.log([one/zero])\n",
        "first_bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt6e2ElcSY2d"
      },
      "source": [
        "Using the first bias above in the baseline model function to make prediction on train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3X5_AkwZz_uc"
      },
      "outputs": [],
      "source": [
        "model = make_model(output_bias=first_bias)\n",
        "model.predict(df_train_features[:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjJbzMDKUlMA"
      },
      "source": [
        "This shows that our loss remains stagnant as there is no change in values of initial loss and returns_later below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HlWk6URdz_ms"
      },
      "outputs": [],
      "source": [
        "returns_later = model.evaluate(df_train_features, df_train_labels, batch_size=batch_size, verbose=0)\n",
        "print(\"Loss: {:0.5f}\".format(returns_later[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QYYZOV01wd0"
      },
      "source": [
        "#Checkpoint the initial weightsÂ¶\n",
        "To make the various training runs more comparable, keep this initial model's weights in a checkpoint file, and load them into each model before training: This statement means the model should be trained and save somewhere. If we need to start it again, we check where it's saved and import it into the system for more training.\n",
        "\n",
        "Before moving on, confirm quick that the careful bias initialization actually helped.\n",
        "\n",
        "Train the model for 20 epochs, with and without this careful initialization, and compare the losses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1G2CyCIrz_ff"
      },
      "outputs": [],
      "source": [
        "first_weights = os.path.join(tempfile.mkdtemp(), 'first_weights')\n",
        "model.save_weights(first_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4tNzsWMV0wR"
      },
      "source": [
        "Assign 0 to be weight of the last layer(-1) and train the data on train data and validate with validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V9Jrnp0T195-"
      },
      "outputs": [],
      "source": [
        "model = make_model()\n",
        "model.load_weights(first_weights)\n",
        "model.layers[-1].bias.assign([0.0])\n",
        "zero_bias_history = model.fit(df_train_features, df_train_labels, batch_size=batch_size, epochs=20, validation_data=(df_val_features, df_val_labels),  verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qyafBEPWjwI"
      },
      "source": [
        "Train the data as above but by assigning1 as weight of the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HkhytbxT19jD"
      },
      "outputs": [],
      "source": [
        "model = make_model()\n",
        "model.load_weights(first_weights)\n",
        "careful_bias_history = model.fit(df_train_features, df_train_labels, batch_size=batch_size, epochs=20,  validation_data=(df_val_features, df_val_labels),  verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJH_6QUNW3MD"
      },
      "source": [
        "Create a plot function to plot zero_bias and creful_bias trainings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lY6ceWmI19Jn"
      },
      "outputs": [],
      "source": [
        "def plot_loss_model(history, label, n):\n",
        "  # Use a log scale on y-axis to show the wide range of values.\n",
        "  plt.semilogy(history.epoch, history.history['loss'], color=colors[n], label='Train ' + label)\n",
        "  plt.semilogy(history.epoch, history.history['val_loss'], color=colors[n], label='Val ' + label, linestyle=\"--\")\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bp_-SrkXcNX"
      },
      "source": [
        "Invoking the function to plot the two biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f9cbYQWa18hZ"
      },
      "outputs": [],
      "source": [
        "plot_loss_model(zero_bias_history, \"Zero Bias\", 0)\n",
        "plot_loss_model(careful_bias_history, \"Careful Bias\", 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aNohju2XuJa"
      },
      "source": [
        "From the checkpoint, baseline model we saved earlier is now eing loaded for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9n8Tx17e17p9"
      },
      "outputs": [],
      "source": [
        "model = make_model()\n",
        "model.load_weights(first_weights)\n",
        "baseline_model_history = model.fit(df_train_features, df_train_labels, batch_size=batch_size, epochs=epochs, callbacks=[early_stopping], validation_data=(df_val_features, df_val_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spyAQhrcYAa_"
      },
      "source": [
        "This function plots model metric function defined earlier: precision,recall, prc, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-7rFsHiG17Yu"
      },
      "outputs": [],
      "source": [
        "def plot_model_metrics(history):\n",
        "  metrics = ['loss', 'prc', 'precision', 'recall']\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
        "    plt.plot(history.epoch, history.history['val_'+metric], color=colors[0], linestyle=\"--\", label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "      plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "      plt.ylim([0.9,1])\n",
        "    else:\n",
        "      plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l6kGy8iw17Nz"
      },
      "outputs": [],
      "source": [
        "plot_model_metrics(baseline_model_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdjCb_erYYsJ"
      },
      "source": [
        "Make pediction with baselime model on both train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zikYNUhx2zIv"
      },
      "outputs": [],
      "source": [
        "train_predictions_baseline = model.predict(df_train_features, batch_size=batch_size)\n",
        "test_predictions_baseline = model.predict(df_test_features, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p95Edw75YkRK"
      },
      "source": [
        "This function plots confusion matrix forthe baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wr1-KAaE2y8o"
      },
      "outputs": [],
      "source": [
        "def plot_cm_model(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(8,8))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('Non-Theft correctly Detected (TN): ', cm[0][0])\n",
        "  print('Non-theft Incorrectly Detected (FP): ', cm[0][1])\n",
        "  print('Theft incorrectly Detected (FN): ', cm[1][0])\n",
        "  print('Theft correctly Detected (TP): ', cm[1][1])\n",
        "  print('Total theft Detections: ', np.sum(cm[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQbtDIWu3A2D"
      },
      "source": [
        "#Evaluate your model on the test dataset and display the results for the metrics you created above:\n",
        "If the model had predicted everything perfectly, this would be a diagonal matrix where values off the main diagonal, indicating incorrect predictions, would be zero. In this case the matrix shows that you have relatively few false positives, meaning that there were relatively few legitimate connections that were incorrectly flagged. However, you would likely want to have even fewer false negatives despite the cost of increasing the number of false positives. This trade off may be preferable because false negatives would allow fraudulent connections to go through, whereas false positives may cause an email to be sent to a customer to ask them to verify their energy card activity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oynQnNxE2ywm"
      },
      "outputs": [],
      "source": [
        "baseline_model_results = model.evaluate(df_test_features, df_test_labels, batch_size=batch_size, verbose=0)\n",
        "for name, value in zip(model.metrics_names, baseline_model_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "plot_cm_model(df_test_labels, test_predictions_baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FxU1D4ZY4pd"
      },
      "source": [
        "This function plots roc curve of the baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UpTnj4w12ykK"
      },
      "outputs": [],
      "source": [
        "def plot_roc_model(name, labels, predictions, **kwargs):\n",
        "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  plt.xlim([-0.5,100])\n",
        "  plt.ylim([40,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM1zPAwb3ijf"
      },
      "source": [
        "This plot is useful because it shows, at a glance, the range of performance the model can reach just by tuning the output threshold.\n",
        "\n",
        "Invoking roc_model function with real arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WMkNRg1d2yaL"
      },
      "outputs": [],
      "source": [
        "plot_roc_model(\"Train Baseline_Model\", df_train_labels, train_predictions_baseline, color=colors[0])\n",
        "plot_roc_model(\"Test Baseline_Model\", df_test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
        "plt.legend(loc='upper right');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5GsvgceZt6F"
      },
      "source": [
        "This function plots prc of the baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mIUH3QNb2yNH"
      },
      "outputs": [],
      "source": [
        "def plot_model_prc(name, labels, predictions, **kwargs):\n",
        "    precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)\n",
        "\n",
        "    plt.plot(precision, recall, label=name, linewidth=2, **kwargs)\n",
        "    plt.xlabel('Precision')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.grid(True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_aspect('equal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbUOLal-37bD"
      },
      "source": [
        "Classifiers often face challenges when trying to maximize both precision and recall, which is especially true when working with imbalanced datasets. It is important to consider the costs of different types of errors in the context of the problem you care about. In this example, a false negative (a fraudulent transaction is missed) may have a financial cost, while a false positive (a transaction is incorrectly flagged as fraudulent) may decrease user happiness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOiZ31ZzaWEQ"
      },
      "source": [
        "Invoking model_prc function with real arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6A32W44i17Da"
      },
      "outputs": [],
      "source": [
        "plot_model_prc(\"Train Baseline\", df_train_labels, train_predictions_baseline, color=colors[0])\n",
        "plot_model_prc(\"Test Baseline\", df_test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
        "plt.legend(loc='lower right');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyLa2ESO4GKO"
      },
      "source": [
        "##Calculate class weights\n",
        "The goal is to identify fraudulent connections, but you don't have very many of those positive samples to work with, so you would want to have the classifier heavily weight the few examples that are available. You can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class (1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4YkThYGC1641"
      },
      "outputs": [],
      "source": [
        "# Scaling by total/2 helps keep the loss to a similar value.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "weight_0 = (1 / zero) * (total / 2.0)\n",
        "weight_1 = (1 / one) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_0, 1: weight_1}\n",
        "\n",
        "print('Weight for class negative (0): {:.2f}'.format(weight_0))\n",
        "print('Weight for class positive (1): {:.2f}'.format(weight_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUoO_S8G4alV"
      },
      "source": [
        "Now try re-training and evaluating the model with class weights to see how that affects the predictions.\n",
        "\n",
        "Using class_weights changes the range of the loss. This may affect the stability of the training depending on the optimizer. Optimizers whose step size is dependent on the magnitude of the gradient, like tf.keras.optimizers.SGD, may fail. The optimizer used here, tf.keras.optimizers.Adam, is unaffected by the scaling change. Also note that because of the weighting, the total losses are not comparable between the two models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ebbQNFqe4RWz"
      },
      "outputs": [],
      "source": [
        "weighted_model = make_model()\n",
        "weighted_model.load_weights(first_weights)\n",
        "\n",
        "weighted_history = weighted_model.fit(df_train_features,df_train_labels,batch_size=batch_size, epochs=epochs, callbacks=[early_stopping], validation_data=(df_val_features, df_val_labels), class_weight=class_weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ciQxiTca4RJH"
      },
      "outputs": [],
      "source": [
        "plot_model_metrics(weighted_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HCeYuBEbBhX"
      },
      "source": [
        "Make prediction with class weight calculate above on both train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2JtRKy254Q9B"
      },
      "outputs": [],
      "source": [
        "train_predictions_weighted = weighted_model.predict(df_train_features, batch_size=batch_size)\n",
        "test_predictions_weighted = weighted_model.predict(df_test_features, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfNaEBy2bYUI"
      },
      "source": [
        "Print results using confusion matrix for class weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2RpAim9P4Q0E"
      },
      "outputs": [],
      "source": [
        "weighted_results = weighted_model.evaluate(df_test_features, df_test_labels, batch_size=batch_size, verbose=0)\n",
        "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "plot_cm_model(df_test_labels, test_predictions_weighted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EILMLNC7bmqJ"
      },
      "source": [
        "Also plot roc for the baseline model and weighted mdel using train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UFh0ROep4Qqj"
      },
      "outputs": [],
      "source": [
        "plot_roc_model(\"Train Baseline_Model\", df_train_labels, train_predictions_baseline, color=colors[0])\n",
        "plot_roc_model(\"Test Baseline_Model\", df_test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
        "plot_roc_model(\"Train Weighted_Model\", df_train_labels, train_predictions_weighted, color=colors[1])\n",
        "plot_roc_model(\"Test Weighted_Model\", df_test_labels, test_predictions_weighted, color=colors[1], linestyle='--')\n",
        "plt.legend(loc='upper right');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfL_Lrcjb_go"
      },
      "source": [
        "Invoking prc function for baseline and weighted models using train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9-gBfMTq4QfZ"
      },
      "outputs": [],
      "source": [
        "plot_model_prc(\"Train Baseline_Model\", df_train_labels, train_predictions_baseline, color=colors[0])\n",
        "plot_model_prc(\"Test Baseline_Model\", df_test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
        "\n",
        "plot_model_prc(\"Train Weighted_Model\", df_train_labels, train_predictions_weighted, color=colors[1])\n",
        "plot_model_prc(\"Test Weighted_Model\", df_test_labels, test_predictions_weighted, color=colors[1], linestyle='--')\n",
        "\n",
        "\n",
        "plt.legend(loc='lower right');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlRe_Z_Z5Ew3"
      },
      "source": [
        "#Oversample the minority class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vsnqyCLn4QRl"
      },
      "outputs": [],
      "source": [
        "one_features = df_train_features[bin_train_labels]      #train data for only where FLAG == 1\n",
        "zero_features = df_train_features[~bin_train_labels]   #train data for only where FLAG == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rJknoWQgzKhQ"
      },
      "outputs": [],
      "source": [
        "one_labels = df_train_labels[bin_train_labels]#train LABELS for only where FLAG == 1\n",
        "zero_labels = df_train_labels[~bin_train_labels]#train LABELS for only where FLAG == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KHAk6Ln75VWj"
      },
      "outputs": [],
      "source": [
        "(one_labels[:20], zero_labels[:20]) #Print first 20 for 1 and 0 labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-PtrPBm5cIg"
      },
      "source": [
        "You can balance the dataset manually by choosing the right number of random indices from the positive examples:Â¶\n",
        "\n",
        "Using numpy random module to shuffle the data and make some choice of 1 and 0 features.\n",
        "\n",
        "This step tries to oversample the minority 1 class to e equal to the majority 0 class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IH9tGx2Z5VK-"
      },
      "outputs": [],
      "source": [
        "ids = np.arange(len(one_features))\n",
        "choices = np.random.choice(ids, len(zero_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HI1IGLPQ5U_s"
      },
      "outputs": [],
      "source": [
        "res_one_features = one_features[choices]\n",
        "res_one_labels = one_labels[choices]\n",
        "\n",
        "res_one_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFn_Fp_sezgv"
      },
      "source": [
        "Adding or concatenating 1 and 0 features together gives us a resampled features of 62010 rows and 1034 columns\n",
        "\n",
        "With tf.data from tensorflow, the easiest way to produce balanced examples is to start with a one and a zero dataset, and concatenate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wxUd97Nd5Uu2"
      },
      "outputs": [],
      "source": [
        "resampled_features = np.concatenate([res_one_features, zero_features], axis=0)\n",
        "resampled_labels = np.concatenate([res_one_labels, zero_labels], axis=0)\n",
        "\n",
        "order = np.arange(len(resampled_labels))\n",
        "np.random.shuffle(order)\n",
        "resampled_features = resampled_features[order]\n",
        "resampled_labels = resampled_labels[order]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JEVmTJ9b5UCD"
      },
      "outputs": [],
      "source": [
        "resampled_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy4ZmblGfxZ8"
      },
      "source": [
        "Using tensorflow, create a function that shuffles our oversampled data and returns a shuffled dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EjPyqThe5TyQ"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 100000\n",
        "\n",
        "def make_ds(features, labels):\n",
        "  ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "  ds = ds.shuffle(BUFFER_SIZE).repeat()\n",
        "  return ds\n",
        "\n",
        "one_ds = make_ds(one_features, one_labels)\n",
        "zero_ds = make_ds(zero_features, zero_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rf_klmMwQ-A"
      },
      "source": [
        "Print a sample of a row from the dataset for feature and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eiR0mrhL5Suq"
      },
      "outputs": [],
      "source": [
        "for features, label in one_ds.take(1):\n",
        "  print(\"Features:\\n\", features.numpy())\n",
        "  print()\n",
        "  print(\"Label: \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQEJJTSQ6OTr"
      },
      "source": [
        "Merge the two together using tf.data.Dataset.sample_from_datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LAc7pRv05Sds"
      },
      "outputs": [],
      "source": [
        "resampled_ds = tf.data.Dataset.sample_from_datasets([one_ds, zero_ds], weights=[0.5, 0.5])\n",
        "resampled_ds = resampled_ds.batch(batch_size).prefetch(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KEZ_7RF65SFz"
      },
      "outputs": [],
      "source": [
        "for features, label in resampled_ds.take(1):\n",
        "  print(label.numpy().mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qctvWsX6iBl"
      },
      "source": [
        "To use this dataset, you'll need the number of steps per epoch.\n",
        "\n",
        "epoch here is the number of batches required to see each negative example once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R7XBewDz5R4A"
      },
      "outputs": [],
      "source": [
        "resampled_steps_per_epoch = np.ceil(2.0*zero/batch_size)\n",
        "resampled_steps_per_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr4HlI0i65ka"
      },
      "source": [
        "## Train on the oversampled data\n",
        "Now try training the model with the resampled data set instead of using class weights to see how these methods compare.\n",
        "\n",
        "Note: Because the data was balanced by replicating the positive examples, the total dataset size is larger, and each epoch runs for more training steps.\n",
        "Invoke baseline model function with oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sc8JwTZK5Run"
      },
      "outputs": [],
      "source": [
        "resampled_model = make_model()\n",
        "resampled_model.load_weights(first_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1WEQ_mSo5Rkp"
      },
      "outputs": [],
      "source": [
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = resampled_model.layers[-1]\n",
        "output_layer.bias.assign([0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oOIj327O7jaZ"
      },
      "outputs": [],
      "source": [
        "val_ds = tf.data.Dataset.from_tensor_slices((df_val_features, df_val_labels)).cache()\n",
        "val_ds = val_ds.batch(batch_size).prefetch(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FCBRAVJ87ICe"
      },
      "outputs": [],
      "source": [
        "resampled_history = resampled_model.fit(resampled_ds, epochs=epochs, steps_per_epoch=resampled_steps_per_epoch, callbacks=[early_stopping], validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAq8H3Ti7t3j"
      },
      "source": [
        "When training the model batch-wise, as you did here, the oversampled data provides a smoother gradient signal: Instead of each positive example being shown in one batch with a large weight, they're shown in many different batches each time with a small weight.\n",
        "\n",
        "This smoother gradient signal makes it easier to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9qWRjPfk7H3D"
      },
      "outputs": [],
      "source": [
        "plot_model_metrics(resampled_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G0vOpVuyMZv"
      },
      "source": [
        "Make prediction with train and test data on oversampled model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PyBLAmG07HQY"
      },
      "outputs": [],
      "source": [
        "train_predictions_resampled = resampled_model.predict(df_train_features, batch_size=batch_size)\n",
        "test_predictions_resampled = resampled_model.predict(df_test_features, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQhiNUkDyr5U"
      },
      "source": [
        "Evaluate the oversampled model with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1MwPfgKR7Geg"
      },
      "outputs": [],
      "source": [
        "resampled_results = resampled_model.evaluate(df_test_features, df_test_labels,  batch_size=batch_size, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbnD2rL1y7hx"
      },
      "source": [
        "Calling plot_cm_model with test data and predictions from oversampled model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8QBZNz0e8Iez"
      },
      "outputs": [],
      "source": [
        "for name, value in zip(model.metrics_names, resampled_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "plot_cm_model(df_test_labels, test_predictions_resampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYrZa4Uz8Wy2"
      },
      "source": [
        "Because training is easier on the balanced data, the above training procedure may overfit quickly.\n",
        "\n",
        "So break up the epochs to give the tf.keras.callbacks.EarlyStopping finer control over when to stop training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1Ao8YL8H8Ht4"
      },
      "outputs": [],
      "source": [
        "resampled_model = make_model()\n",
        "resampled_model.load_weights(first_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qc2wLoMz0-E"
      },
      "source": [
        "Increase number of epochs by multiplying by 10 = 100*10 and train the oversampled model again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gM1i_9bl8Hdp"
      },
      "outputs": [],
      "source": [
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = resampled_model.layers[-1]\n",
        "output_layer.bias.assign([0])\n",
        "resampled_history = resampled_model.fit(resampled_ds, steps_per_epoch=20, epochs=10*epochs, callbacks=[early_stopping], validation_data=(val_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sxod9Fd0Ubh"
      },
      "source": [
        "Re-plot the metrics for the re-trained oversampled model above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "okAPwr2Y8HRB"
      },
      "outputs": [],
      "source": [
        "plot_model_metrics(resampled_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2GhRe0T0kOT"
      },
      "source": [
        "Make prediction for the oversampled model using train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hho9ofsD8G2r"
      },
      "outputs": [],
      "source": [
        "train_predictions_resampled = resampled_model.predict(df_train_features, batch_size=batch_size)\n",
        "test_predictions_resampled = resampled_model.predict(df_test_features, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3pZetZB1IyP"
      },
      "source": [
        "Evaluate the re-oversampled model with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W8YNL5ze8Glo"
      },
      "outputs": [],
      "source": [
        "resampled_results = resampled_model.evaluate(df_test_features, df_test_labels, batch_size=batch_size, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEzdP3WK1iDq"
      },
      "source": [
        "Print the results of all metrics for the re-oversampled model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QS4MPj378GU2"
      },
      "outputs": [],
      "source": [
        "for name, value in zip(resampled_model.metrics_names, resampled_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "plot_cm_model(df_test_labels, test_predictions_resampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBIRBPgW1zSm"
      },
      "source": [
        "Invoke plot_roc_model function on predictions with train data of baseline and test data of  baseline model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KFRsqAaq5RVO"
      },
      "outputs": [],
      "source": [
        "plot_roc_model(\"Train Baseline_Model\", df_train_labels, train_predictions_baseline, color=colors[0])\n",
        "plot_roc_model(\"Test Baseline_Model\", df_test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
        "plt.legend(loc='lower right');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUPlO9PE2rK4"
      },
      "source": [
        "Invoke plot_roc_model function on train data of the class weight and of the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ckTWhiSa8_xH"
      },
      "outputs": [],
      "source": [
        "plot_roc_model(\"Train Weighted_Model\", df_train_labels, train_predictions_weighted, color=colors[1])\n",
        "plot_roc_model(\"Test Weighted_Model\", df_test_labels, test_predictions_weighted, color=colors[1], linestyle='--')\n",
        "plt.legend(loc='lower right');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK-uDCsg3TFz"
      },
      "source": [
        "Invoke plot_roc_model function on train data of the resampled and of the test data of the resampled model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2J9BZX3c8-iR"
      },
      "outputs": [],
      "source": [
        "plot_roc_model(\"Train Resampled_Model\", df_train_labels, train_predictions_resampled, color=colors[2])\n",
        "plot_roc_model(\"Test Resampled_Model\", df_test_labels, test_predictions_resampled, color=colors[2], linestyle='--')\n",
        "plt.legend(loc='lower right');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V08NzI0D4CYT"
      },
      "source": [
        "Invoke plot_model_prc on the predictions of baseline model and train data and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cIt7xtuo8-Go"
      },
      "outputs": [],
      "source": [
        "plot_model_prc(\"Train Baseline_Model\", df_train_labels, train_predictions_baseline, color=colors[0])\n",
        "plot_model_prc(\"Test Baseline_Model\", df_test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
        "plt.legend(loc='lower right');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vq5_tQMw9lng"
      },
      "outputs": [],
      "source": [
        "plot_model_prc(\"Train Weighted_Model\", df_train_labels, train_predictions_weighted, color=colors[1])\n",
        "plot_model_prc(\"Test Weighted_Model\", df_test_labels, test_predictions_weighted, color=colors[1], linestyle='--')\n",
        "plt.legend(loc='lower right');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C8mfSpGy9mAM"
      },
      "outputs": [],
      "source": [
        "plot_model_prc(\"Train Resampled_Model\", df_train_labels, train_predictions_resampled, color=colors[2])\n",
        "plot_model_prc(\"Test Resampled_Model\", df_test_labels, test_predictions_resampled, color=colors[2], linestyle='--')\n",
        "plt.legend(loc='lower right');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gv3eLDTN9mNv"
      },
      "outputs": [],
      "source": [
        "# Load the row of data you want to test\n",
        "row_to_test = df.iloc[7]  # Assuming you want to test the first row\n",
        "\n",
        "# Preprocess the row of data\n",
        "# Extract features and scale them\n",
        "test_features = row_to_test.drop('FLAG')  # Assuming 'FLAG' is the target variable\n",
        "test_features = np.array(test_features).reshape(1, -1)  # Reshape to match the model's input shape\n",
        "test_features = scale_features.transform(test_features)  # Scale features using the same scaler used in training\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predictions = model.predict(test_features)\n",
        "\n",
        "# Interpret the predictions\n",
        "predicted_class = 'Fraudulent' if predictions[0] >= 0.5 else 'Non-Fraudulent'\n",
        "print(f\"The predicted class for the row is: {predicted_class}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBDF/VPN46JAiR3S5CkPsv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}